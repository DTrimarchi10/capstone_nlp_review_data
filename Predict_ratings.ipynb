{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import ast\n",
    "import re\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Tokenized Reviews from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59274, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [grits, know, best, so, take, it, from, this, ...\n",
       "1    [brunch, was, enjoyable, from, the, mimosa, st...\n",
       "2    [if_you're_looking, for, a, delicious, meal, o...\n",
       "3    [the, best, way, i, can, describe, this_place,...\n",
       "4    [had, dinner, here, last_night, and, i'm, stil...\n",
       "Name: review_tokenized, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df = pd.read_csv(\"reviews_all_tokenized.csv\")\n",
    "\n",
    "#Reviews are stored as one string. Need to convert to a list of strings.\n",
    "reviews_df.review_tokenized = reviews_df.review_tokenized.map(ast.literal_eval)\n",
    "\n",
    "print(reviews_df.shape)\n",
    "reviews_df.review_tokenized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define remove phrases function in the event that phrase removal is desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_clear(text_block):\n",
    "    \"\"\"\n",
    "    Removes underscores from tokens in tokenized data. Returns re-tokenized text.\n",
    "    INPUT:\n",
    "    text_block = tokenized text block containing n-grams to be removed.\n",
    "    OUTPUT:\n",
    "    Tokenized text block that has n-grams transformed back into individual words.\n",
    "    \"\"\"\n",
    "    cleared = []\n",
    "    for idx, word in enumerate(text_block):\n",
    "        cleared.extend(word.split(\"_\"))\n",
    "    return cleared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = reviews_df.review_tokenized\n",
    "target = reviews_df.star_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Word Vectors: 65.74 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Generate Word Vectors: \", end='')\n",
    "start = time.time()\n",
    "model = gensim.models.Word2Vec(data,size=100,window=5,min_count=1,workers=4)\n",
    "model.train(data,total_examples=model.corpus_count,epochs=10)\n",
    "end = time.time()\n",
    "print(round(end-start,2),\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63025 unique words in the dataset.\n"
     ]
    }
   ],
   "source": [
    "#Save embedded word vector space\n",
    "wv = model.wv\n",
    "print(len(wv.vocab),\"unique words in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Mean Word Embeddings using Mean Embedding Vectorizer class\n",
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.w2v = model.wv\n",
    "        self.dimensions = model.vector_size\n",
    "    \n",
    "    # Need to implement a fit method as required for sklearn Pipeline.\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v.get_vector(w) for w in words], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dennistrimarchi/anaconda3/envs/learn-env/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "rf =  Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(wv)),\n",
    "              (\"Random Forest\", RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(wv)),\n",
    "                ('Support Vector Machine', SVC(gamma='auto'))])\n",
    "lr = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(wv)),\n",
    "              ('Logistic Regression', LogisticRegression(multi_class='auto'))])\n",
    "\n",
    "models = [('Random Forest', rf),\n",
    "          (\"Support Vector Machine\", svc),\n",
    "          (\"Logistic Regression\", lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   31.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   30.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "/Users/dennistrimarchi/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/dennistrimarchi/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600.99 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.5499374790351649),\n",
       " ('Support Vector Machine', 0.5982049244306866),\n",
       " ('Logistic Regression', 0.5882006478752827)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Cross Validation Scores: \", end='')\n",
    "start = time.time()\n",
    "scores = [(name, cross_val_score(model, data, target, cv=2).mean()) for name, model, in models]\n",
    "end = time.time()\n",
    "print(round(end-start,2),\"seconds\")\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Ratings using GloVe Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using GloVe Vectors\n",
    "glove = {}\n",
    "with open('glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Mean Word Embeddings using Mean Embedding Vectorizer class\n",
    "class W2vVectorizer_glove(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note from Mike: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # It can't be used in a sklearn Pipeline. \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf =  Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer_glove(glove)),\n",
    "              (\"Random Forest\", RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer_glove(glove)),\n",
    "                ('Support Vector Machine', SVC(gamma='auto'))])\n",
    "lr = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer_glove(glove)),\n",
    "              ('Logistic Regression', LogisticRegression(multi_class='auto'))])\n",
    "\n",
    "models = [('Random Forest', rf),\n",
    "          (\"Support Vector Machine\", svc),\n",
    "          (\"Logistic Regression\", lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   23.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   21.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "/Users/dennistrimarchi/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/dennistrimarchi/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416.65 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.4827917019687721),\n",
       " ('Support Vector Machine', 0.46592108185976805),\n",
       " ('Logistic Regression', 0.5072376697259633)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Cross Validation Scores: \", end='')\n",
    "start = time.time()\n",
    "scores = [(name, cross_val_score(model, data, target, cv=2).mean()) for name, model, in models]\n",
    "end = time.time()\n",
    "print(round(end-start,2),\"seconds\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings - Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(target).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = text.Tokenizer(num_words=20000)\n",
    "#tokenizer.fit_on_texts(list(reviews_df.review))\n",
    "#tokenized_reviews = tokenizer.texts_to_sequences(reviews_df.review)\n",
    "#X_t = sequence.pad_sequences(tokenized_reviews, maxlen=100)\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "#tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n",
    "tokenized_reviews = tokenizer.texts_to_sequences(data)\n",
    "X_t = sequence.pad_sequences(tokenized_reviews, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "input_ = Input(shape=(100,))\n",
    "x = Embedding(20000, embedding_size)(input_)\n",
    "x = LSTM(25, return_sequences=True)(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# There are 41 different possible classes, so we use 41 neurons in our output layer\n",
    "x = Dense(5, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 100, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100, 25)           15400     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                1300      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 2,576,955\n",
      "Trainable params: 2,576,955\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53346 samples, validate on 5928 samples\n",
      "Epoch 1/5\n",
      "53346/53346 [==============================] - 132s 2ms/step - loss: 0.7362 - acc: 0.6876 - val_loss: 1.0945 - val_acc: 0.5757\n",
      "Epoch 2/5\n",
      "53346/53346 [==============================] - 115s 2ms/step - loss: 0.6746 - acc: 0.7202 - val_loss: 1.1868 - val_acc: 0.5523\n",
      "Epoch 3/5\n",
      "53346/53346 [==============================] - 116s 2ms/step - loss: 0.6144 - acc: 0.7507 - val_loss: 1.2449 - val_acc: 0.5700\n",
      "Epoch 4/5\n",
      "53346/53346 [==============================] - 109s 2ms/step - loss: 0.5675 - acc: 0.7741 - val_loss: 1.2685 - val_acc: 0.5646\n",
      "Epoch 5/5\n",
      "53346/53346 [==============================] - 109s 2ms/step - loss: 0.5185 - acc: 0.7942 - val_loss: 1.5046 - val_acc: 0.5442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a59631ba8>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y, epochs=5, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
